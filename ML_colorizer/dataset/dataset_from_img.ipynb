{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c543d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from local_paths import *\n",
    "\"\"\"\n",
    "local_paths defines local paths to the required folders,\n",
    "the variables used for the paths are listed below:\n",
    "\n",
    "IMAGE_DATASET_128_DIR\n",
    "IMAGE_DATASET_256_DIR\n",
    "OUTPUT_DATASET_DIR_128\n",
    "OUTPUT_DATASET_DIR_256\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator(image_size: typing.Tuple[int, int],\n",
    "                      images_dir: bytes):\n",
    "    \n",
    "    images_dir = str(images_dir).split('\\'')[1]\n",
    "    # it is not specified what encoding is used, or I did not find it\n",
    "\n",
    "    images_paths = [os.path.join(images_dir, name)\n",
    "                    for name in os.listdir(images_dir)]\n",
    "    for path in images_paths:\n",
    "        image = cv2.imread(path)\n",
    "\n",
    "\n",
    "        height, width, *_ = image.shape\n",
    "        if (np.all((height, width) != image_size)):\n",
    "            image = cv2.resize(image.astype(np.float32), image_size)\n",
    "\n",
    "\n",
    "        image = cv2.cvtColor(image.astype(np.float32), cv2.COLOR_BGR2LAB)\n",
    "\n",
    "        # normalization of L-channel (0, 255) -> (0, 1)\n",
    "        l_channel = image[:, :, 0] / 255.0\n",
    "        # add dimension (h, w) -> (h, w, 1)\n",
    "        l_channel = (l_channel[..., np.newaxis]).astype(np.float32)\n",
    "\n",
    "        # normalization (0, 255) -> (-1, 1)\n",
    "        ab_channel = ((image[:, :, 1:] - 128.0) / 128.0).astype(np.float32)\n",
    "\n",
    "        yield tf.convert_to_tensor(l_channel), tf.convert_to_tensor(ab_channel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a40fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (128, 128)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(dataset_generator,\n",
    "                                         args=(IMAGE_SIZE, IMAGE_DATASET_128_DIR), # converts strings to bytes\n",
    "                                         output_signature=(tf.TensorSpec(shape=IMAGE_SIZE + (1,), dtype=tf.float32, name=None),\n",
    "                                                           tf.TensorSpec(shape=IMAGE_SIZE + (2,), dtype=tf.float32, name=None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96e160f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save(OUTPUT_DATASET_DIR_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "475b2769",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(dataset_generator,\n",
    "                                         args=(IMAGE_SIZE, IMAGE_DATASET_256_DIR),\n",
    "                                         output_signature=(tf.TensorSpec(shape=IMAGE_SIZE + (1,), dtype=tf.float32, name=None),\n",
    "                                                           tf.TensorSpec(shape=IMAGE_SIZE + (2,), dtype=tf.float32, name=None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68e9279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save(OUTPUT_DATASET_DIR_256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
