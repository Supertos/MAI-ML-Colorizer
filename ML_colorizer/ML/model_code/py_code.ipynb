{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.api import layers\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from keras.api.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# import time\n",
    "\n",
    "training_completed = True\n",
    "ds_path = '/DATA_128/'\n",
    "ds_path1 = '/DATA_128/'\n",
    "train_dataset = tf.data.Dataset.load(ds_path + 'train')\n",
    "test_dataset = tf.data.Dataset.load(ds_path1 + 'test')\n",
    "tmp = train_dataset.cardinality()\n",
    "\n",
    "train_dataset = train_dataset.batch(64)\n",
    "test_dataset = test_dataset.batch(64)\n",
    "train_dataset = train_dataset.cache()\n",
    "test_dataset = test_dataset.cache()\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.shuffle(train_dataset.cardinality())\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.prefetch(5)\n",
    "test_dataset = test_dataset.prefetch(5)\n",
    "\n",
    "\n",
    "# print(train_dataset.cardinality())\n",
    "# print(test_dataset.cardinality())\n",
    "# print(train_dataset.element_spec)\n",
    "# print(test_dataset.element_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "SHAPE_ = (128, 128, 1)\n",
    "L1_NORM = 1e-5 * 0.0\n",
    "L2_NORM = 1e-6 * 0.0\n",
    "\n",
    "def encoder_block(filters: int , kernel_size: Tuple[int, int], apply_batch_normalization = True, l1_reg = 0.0, l2_reg = 0.0):\n",
    "    downsample = keras.models.Sequential()\n",
    "    downsample.add(layers.Conv2D(filters, kernel_size, padding = 'same', strides = 2,\n",
    "                                 kernel_regularizer=keras.regularizers.L1L2(l1=l1_reg, l2=l2_reg)))\n",
    "    if apply_batch_normalization:\n",
    "        downsample.add(layers.BatchNormalization())\n",
    "    downsample.add(keras.layers.LeakyReLU())\n",
    "    return downsample\n",
    "\n",
    "def decoder_block(filters: int, kernel_size: Tuple[int, int], dropout = False, l1_reg = 0.0, l2_reg = 0.0):\n",
    "    upsample = keras.models.Sequential()\n",
    "    upsample.add(layers.Conv2DTranspose(filters, kernel_size, padding = 'same', strides = 2,\n",
    "                                        kernel_regularizer=keras.regularizers.L1L2(l1=l1_reg, l2=l2_reg)))\n",
    "    if dropout:\n",
    "        upsample.add(layers.Dropout(0.2))\n",
    "    upsample.add(keras.layers.LeakyReLU())\n",
    "    return upsample\n",
    "\n",
    "def build_colorizer(input_shape = SHAPE_, l2_reg = L2_NORM, l1_reg = L1_NORM):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # ENCODER\n",
    "\n",
    "    x1 = encoder_block(128, (3, 3), False)(inputs) # /2\n",
    "    x2 = encoder_block(128, (3, 3), False) (x1) # /4\n",
    "    x3 = encoder_block(256, (3, 3), True) (x2) # /8\n",
    "    x4 = encoder_block(512, (3, 3), True) (x3) # /16\n",
    "    x5 = encoder_block(1024, (3, 3), True) (x4) # /32\n",
    "    x6 = encoder_block(2048, (3, 3), True) (x5) # /64\n",
    "    # LATENT SPACE\n",
    "\n",
    "    b1 = encoder_block(2048, (3, 3), True) (x6) # /128\n",
    "\n",
    "    # DECODER\n",
    "\n",
    "    y6 = decoder_block(2048, (3, 3), False) (b1)\n",
    "    y6 = layers.concatenate([y6, x6])\n",
    "\n",
    "    y5 = decoder_block(1024, (3, 3), False) (y6)\n",
    "    y5 = layers.concatenate([y5, x5])\n",
    "\n",
    "    y4 = decoder_block(512, (3, 3), False) (y5)\n",
    "    y4 = layers.concatenate([y4, x4])\n",
    "\n",
    "    y3 = decoder_block(256, (3, 3), False) (y4)\n",
    "    y3 = layers.concatenate([y3, x3])\n",
    "\n",
    "    y2 = decoder_block(128, (3, 3), False) (y3)\n",
    "    y2 = layers.concatenate([y2, x2])\n",
    "\n",
    "    y1 = decoder_block(128, (3, 3), False) (y2)\n",
    "    y1 = layers.concatenate([y1, x1])\n",
    "\n",
    "\n",
    "    outputs = decoder_block(2, (3, 3), False) (y1)\n",
    "    outputs = layers.concatenate([outputs, inputs])\n",
    "    outputs = layers.Conv2D(2, (3,3), padding='same', strides=1, activation='tanh', kernel_initializer=keras.initializers.GlorotNormal()) (outputs)\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "SHAPE_ = (128, 128, 1)\n",
    "L1_NORM = 1e-5 * 0.0\n",
    "L2_NORM = 1e-6 * 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "colorizer = build_colorizer(SHAPE_, L2_NORM)\n",
    "\n",
    "\n",
    "\n",
    "if training_completed:\n",
    "    colorizer = build_colorizer(SHAPE_, L2_NORM)\n",
    "    _path = '/MK/5_128_2048_checkpoint.weights.h5' \n",
    "    colorizer.load_weights(_path)\n",
    "else:\n",
    "    colorizer.compile(optimizer=keras.optimizers.Adam(learning_rate = 0.001),\n",
    "                  loss=keras.losses.mean_squared_error)\n",
    "    hist = colorizer.fit(\n",
    "        train_dataset,\n",
    "        epochs=300,\n",
    "        validation_data=test_dataset,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor='val_loss', patience=10, min_delta= 0.00005),\n",
    "            ModelCheckpoint('/MK/128_2048_checkpoint.weights.h5',\n",
    "                        save_best_only=True, save_weights_only=True)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.plot(hist.history['loss'][0:], label = f'loss-{L2_NORM}')\n",
    "plt.plot(hist.history['val_loss'][0:], label=f'val loss-{L2_NORM}')\n",
    "plt.title(\"Loss vs Val_Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.savefig(f'L2-{L2_NORM}.png')\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN IMAGE\n",
    "def show_dataset_example():\n",
    "    for l_batch, ab_batch in train_dataset.take(5):\n",
    "        l_batch = tf.expand_dims(l_batch[0], axis=0)\n",
    "        print(type(l_batch))\n",
    "        print(l_batch.shape)\n",
    "        l_batch\n",
    "        pred_ab = colorizer.predict(l_batch)[0]\n",
    "        # print(pred_ab.shape)\n",
    "        # print(ab_batch[0][0][0])\n",
    "        # print(l_batch[0][0][0])\n",
    "\n",
    "        l_denorm = (l_batch[0].numpy() * 256.0).astype(np.uint8)\n",
    "        tf.image.flip_left_right(ab_batch)\n",
    "        ab_denorm = ((ab_batch[0].numpy()) * 128.0 + 128.0).astype(np.uint8)\n",
    "        pred_ab_denorm = ((pred_ab ) * 128.0 + 128.0).astype(np.uint8)\n",
    "        print(pred_ab[0][0])\n",
    "        l_denorm2 = (l_batch[0].numpy() * 256.0).astype(np.uint8)\n",
    "\n",
    "        original_lab = np.concatenate([l_denorm2, ab_denorm], axis=-1)\n",
    "        pred_lab = np.concatenate([l_denorm2, pred_ab_denorm], axis=-1)\n",
    "\n",
    "\n",
    "        original_rgb = cv2.cvtColor(original_lab, cv2.COLOR_LAB2RGB)\n",
    "        pred_rgb = cv2.cvtColor(pred_lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(l_denorm.squeeze(), cmap='gray')\n",
    "        plt.title('Input (L channel)')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(original_rgb)\n",
    "        plt.title('Real')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred_rgb)\n",
    "        plt.title('Prediction')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "show_dataset_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# TEST IMAGE\n",
    "\n",
    "def show_dataset_example():\n",
    "    for l_batch, ab_batch in test_dataset.take(5):\n",
    "        l_batch = tf.expand_dims(l_batch[0], axis=0)\n",
    "        print(type(l_batch))\n",
    "        print(l_batch.shape)\n",
    "        l_batch\n",
    "        pred_ab = colorizer.predict(l_batch)[0]\n",
    "        # print(pred_ab.shape)\n",
    "        # print(ab_batch[0][0][0])\n",
    "        # print(l_batch[0][0][0])\n",
    "        l_denorm = (l_batch[0].numpy() * 256.0).astype(np.uint8)\n",
    "        tf.image.flip_left_right(ab_batch)\n",
    "        ab_denorm = ((ab_batch[0].numpy()) * 128.0 + 128.0).astype(np.uint8)\n",
    "        pred_ab_denorm = ((pred_ab ) * 128.0 + 128.0).astype(np.uint8)\n",
    "        print(pred_ab[0][0])\n",
    "        l_denorm2 = (l_batch[0].numpy() * 256.0).astype(np.uint8)\n",
    "\n",
    "        original_lab = np.concatenate([l_denorm2, ab_denorm], axis=-1)\n",
    "        pred_lab = np.concatenate([l_denorm2, pred_ab_denorm], axis=-1)\n",
    "\n",
    "        original_rgb = cv2.cvtColor(original_lab, cv2.COLOR_LAB2RGB)\n",
    "        pred_rgb = cv2.cvtColor(pred_lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(l_denorm.squeeze(), cmap='gray')\n",
    "        plt.title('Input (L channel)')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(original_rgb)\n",
    "        plt.title('Real')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred_rgb)\n",
    "        plt.title('Prediction')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "show_dataset_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "colorizer = build_colorizer(SHAPE_, L2_NORM)\n",
    "# colorizer.compile(optimizer=keras.optimizers.Adam(), loss = keras.losses.MeanSquaredError()) #tf.keras.losses.MSLE\n",
    "colorizer.compile(optimizer=keras.optimizers.SGD(learning_rate=0.1, weight_decay=1e-6, momentum=0.0, nesterov=True),\n",
    "                  loss=keras.losses.mean_squared_error)\n",
    "\n",
    "colorizer.summary()\n",
    "keras.utils.plot_model(colorizer, show_shapes=True, show_trainable=True, dpi=64)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
